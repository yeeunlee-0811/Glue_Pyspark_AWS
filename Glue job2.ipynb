{"cells":[{"cell_type":"code","source":["import sys\nfrom pyspark.sql.functions import when\nfrom pyspark.sql import SparkSession\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16b57620-87f9-4755-ab9a-ec01361128b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["args = getResolvedOptions(sys.argv,['s3_target_path_key','s3_target_path_bucket'])\nbucket = args['s3_target_path_key']\nfileName = args['s3_target_path_bucket']\n\nprint(bucket, fileName)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3643d6cf-a523-4a73-b311-f63cededfd18","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"CDC2\").getOrCreate()\ninputFilePath = f\"s3a://{bucket}/{fileName}\"\nfinalFilePath = f\"s3a://cdc-output-pyspark/output\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7512698a-3742-4530-a4ff-d6165266947a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if \"LOAD\" in fileName:\n    df0 = spark.read.csv(inputFilePath)\n    df0 = df0.withColumnRenamed(\"_c0\",\"id\").withColumnRenamed(\"_c1\",\"FullName\").withColumnRenamed(\"_c2\",\"Location\")\n    df0.write.mode(\"overwrite\").csv(finalFilePath)\nelse:\n    u_df = spark.read.csv(inputFilePath)\n    u_df = u_df.withColumnRenamed(\"_c0\",\"action\").withColumnRenamed(\"_c1\",\"id\").withColumnRenamed(\"_c2\",\"FullName\").withColumnRenamed(\"_c3\",\"Location\")\n    r_df = spark.read.csv(finalFilePath)\n    r_df = r_df.withColumnRenamed(\"_c0\",\"id\").withColumnRenamed(\"_c1\",\"FullName\").withColumnRenamed(\"_c2\",\"Location\")\n    \n    for row in u_df.collect():\n        if row[\"action\"] == \"U\":\n            r_df = r_df.withColumn(\"FullName\", when(r_df[\"id\"] == row[\"id\"], row[\"FullName\"]).otherwise(r_df[\"FullName\"]))\n            r_df = r_df.withColumn(\"Location\", when(r_df[\"id\"] == row[\"id\"], row[\"Location\"]).otherwise(r_df[\"Location\"]))\n        \n        if row[\"action\"] == \"I\":\n            newrow = [list(row)[1:]]\n            columns = ['id', 'FullName','Location']\n            inserted_df = spark.createDataFrame(newrow, columns)\n            r_df = r_df.union(newrow)\n        \n        if row[\"action\"] == \"D\":\n            r_df = r_df.filter(r_df.id != row[\"id\"])\n    \n    r_df.write.mode(\"overwrite\").csv(finalFilePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4a7abd9-510b-4d0a-b14d-614f83b9cd70","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Glue job2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4212058287171646}},"nbformat":4,"nbformat_minor":0}
